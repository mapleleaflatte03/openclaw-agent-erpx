#!/usr/bin/env python3
"""Generate benchmark report from results JSON.

Outputs Markdown and optionally CSV.

Usage:
  python report_benchmark.py --in reports/benchmark/latest.json --out-md reports/benchmark/latest.md
"""
from __future__ import annotations

import argparse
import csv
import json
import sys
from io import StringIO
from pathlib import Path

# Import scorer
sys.path.insert(0, str(Path(__file__).parent))
from score import score  # noqa: E402


def _render_md(results: dict, scores: dict) -> str:
    """Render benchmark report as Markdown."""
    lines = [
        "# Benchmark Report — OpenClaw Agent ERPX",
        "",
        f"**Timestamp**: {results.get('timestamp', 'N/A')}",
        f"**Target**: {results.get('target', 'N/A')}",
        f"**Cases**: {scores.get('total_cases', 0)} "
        f"(failed: {scores.get('failed_cases', 0)}, skipped: {scores.get('skipped_cases', 0)})",
        "",
        "## KPI Summary",
        "",
        "| Metric | Value | Threshold | Status |",
        "| --- | --- | --- | --- |",
        f"| Accuracy | {scores['accuracy']:.4f} | ≥ 0.85 | {'✅ PASS' if scores['accuracy'] >= 0.85 else '❌ FAIL'} |",
        f"| Fail Rate | {scores['fail_rate']:.4f} | < 0.05 | {'✅ PASS' if scores['fail_rate'] < 0.05 else '❌ FAIL'} |",
        f"| Avg Latency | {scores['avg_latency']:.2f}s | report | - |",
        f"| P95 Latency | {scores['p95_latency']:.2f}s | report | - |",
        f"| Workflow Pass Rate | {scores['workflow_pass_rate']:.4f} | report | - |",
        "",
        "## Workflow Results",
        "",
        "| Workflow | Status | Duration (s) |",
        "| --- | --- | --- |",
    ]

    for wr in results.get("workflow_results", []):
        lines.append(
            f"| {wr.get('workflow', 'N/A')} | {wr.get('status', 'N/A')} | {wr.get('duration_s', 'N/A')} |"
        )

    lines += [
        "",
        "## Case Results (top errors)",
        "",
        "| Case | Status | Duration (s) | Truth Obls | Detected Obls | Error |",
        "| --- | --- | --- | --- | --- | --- |",
    ]

    # Show failed/error cases first, then a sample of successful ones
    case_results = results.get("case_results", [])
    errors = [r for r in case_results if r.get("status") in ("failed", "error")]
    successes = [r for r in case_results if r.get("status") not in ("failed", "error", "skip")]

    for r in errors[:20]:
        lines.append(
            f"| {r.get('case_id', '')} | {r.get('status', '')} | {r.get('duration_s', '')} "
            f"| {r.get('truth_obligations', '')} | {r.get('detected_obligations', '')} "
            f"| {r.get('error', '')[:50]} |"
        )

    for r in successes[:10]:
        lines.append(
            f"| {r.get('case_id', '')} | {r.get('status', '')} | {r.get('duration_s', '')} "
            f"| {r.get('truth_obligations', '')} | {r.get('detected_obligations', '')} | - |"
        )

    if len(successes) > 10:
        lines.append(f"| ... | ({len(successes) - 10} more successful) | | | | |")

    lines += ["", "---", "Generated by `report_benchmark.py`"]
    return "\n".join(lines)


def _render_csv(results: dict) -> str:
    """Render case results as CSV."""
    output = StringIO()
    writer = csv.DictWriter(output, fieldnames=[
        "case_id", "status", "duration_s", "truth_obligations",
        "detected_obligations", "expected_risk", "expected_gating_tier",
        "expected_approvals", "error",
    ])
    writer.writeheader()
    for r in results.get("case_results", []):
        writer.writerow({
            "case_id": r.get("case_id", ""),
            "status": r.get("status", ""),
            "duration_s": r.get("duration_s", ""),
            "truth_obligations": r.get("truth_obligations", ""),
            "detected_obligations": r.get("detected_obligations", ""),
            "expected_risk": r.get("expected_risk", ""),
            "expected_gating_tier": r.get("expected_gating_tier", ""),
            "expected_approvals": r.get("expected_approvals", ""),
            "error": r.get("error", ""),
        })
    return output.getvalue()


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate benchmark report")
    parser.add_argument("--in", dest="input", type=str, required=True)
    parser.add_argument("--out-md", type=str, default="reports/benchmark/latest.md")
    parser.add_argument("--out-csv", type=str, default=None)
    args = parser.parse_args()

    input_path = Path(args.input)
    if not input_path.exists():
        print(f"ERROR: {input_path} not found", file=sys.stderr)
        sys.exit(1)

    results = json.loads(input_path.read_text())
    scores = score(results)

    md_path = Path(args.out_md)
    md_path.parent.mkdir(parents=True, exist_ok=True)
    md_content = _render_md(results, scores)
    md_path.write_text(md_content, encoding="utf-8")
    print(f"Markdown report: {md_path}")

    if args.out_csv:
        csv_path = Path(args.out_csv)
        csv_path.parent.mkdir(parents=True, exist_ok=True)
        csv_path.write_text(_render_csv(results), encoding="utf-8")
        print(f"CSV report: {csv_path}")

    # Print summary
    print(f"\n  accuracy={scores['accuracy']}, fail_rate={scores['fail_rate']}")
    gate = "PASS" if scores["accuracy"] >= 0.85 and scores["fail_rate"] < 0.05 else "FAIL"
    print(f"  KPI gate: {gate}")


if __name__ == "__main__":
    main()
